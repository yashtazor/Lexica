# Lexica
Lexica is a text scraper and analyzer made with various NLP, ML, and DL algorithms.

## The Scraper
Lexica uses a pre-made scraper which can only be used to scrape [this](https://sive.rs/book) site. This site was chosen keeping in mind the type of data required by the Analyzer Module for use. All in all, the following fields were scraped and saved in a CSV file

* Name
* Author
* ISBN
* Rating
* Text
* Summary

## The Analyzer

This module works on the CSV file generated by the scraper as the main input and implements three models on that data. They are described below

### 1. Plagiarism Detection 
This utilizes the **doc2vec** implementation of the **Gensim** library to detect similarities between the documents. It generates a similarity score between the documents and plots it on a graph so that we can visually see the most plagiarised documents. An example of such a graph would be like as follows

| ![1](https://user-images.githubusercontent.com/42903859/133926307-08667970-05e4-472b-a698-a34d63bd13ed.png) |
| - |

### 2. Similar Document Clustering
This simply uses the similarity scores generated by the previous model and then uses **K-Means Clustering** implementation of the **Sklearn** library to cluster similar documents. It also displays the clusters in a picture format like as follows

| ![2](https://user-images.githubusercontent.com/42903859/133926442-1185cf53-adae-4115-a71d-7697540c8b9f.png) |
| - |

### 3. Abstractive Text Summarization
This uses an **Encoder-Decoder RNN** with a **Custom Attention Layer** to generate automatic summaries from the data fed into the model. It is a common implementation of the **seq2seq** model. The RNN was implemented using **Keras**.

<b> <p align = "center"> Created by Yash Dekate. </p> </b>
